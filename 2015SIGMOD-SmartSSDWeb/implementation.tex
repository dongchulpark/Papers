
\section{Implementation}\label{sec:implementation}
In this section, we provide the implementation details for offloading the query operations into Smart SSD. We discuss the intersection in Section~\ref{sec:intersection}, union in Section~\ref{sec:union} and difference in Section~\ref{sec:difference}. Finally, we discuss the implementation of ranking in Section~\ref{sec:ranked}.

\textbf{Inverted index format}. As all the operations are over inverted lists, we explain the format first.
Normally, each contains a list of entries, each entry contain document ID, \emph{document frequency} and possibly positions~\cite{ZM06,M08}. In this work, we did a minor change to replace the document frequency by the actual \emph{score} according to the Okapi BM25 model~\cite{Robertson1994}. We explain more in Section~\ref{sec:ranked}. Thus each entry in the list contains: document ID (8 bytes), score (8 bytes) and positional information (fixed for 16 bytes)\footnote{In practice, the positional information varies. However, in this work, we consider the fixed-length entry in order to use binary search for skipping (otherwise need to build some auxiliary data structure, e.g., skip list~\cite{Pugh1990}.)}. And, each inverted list is sorted by the document ID, and not compressed. Although compressed lists save space, decompression takes considerable time inside Smart SSD due to the hardware limit. We are improving the performance of Smart SSD (both hardware and software), and will consider the compressed list operations in the future work. Besides that, each inverted list is stored on SSD in a page-aligned manner (page size 8KB) due to the hardware constraints.


\subsection{Intersection}\label{sec:intersection}

Suppose there are $k$ ($k>1$) inverted lists, i.e., $L_1$, $\cdots$, $L_k$, for intersection. Initially, there are stored on SSD. Thus, we need to load them to memory first. Then apply an in-memory list intersection algorithm. We consider both I/O time and CPU time.

There are a number of in-memory list intersection algorithms, e.g., sort-merge based algorithm~\cite{GarciaMolina2008}, skip list based algorithm~\cite{M08}, hash based algorithm, divide and conquer based algorithm~\cite{Baezayates05}, adaptive algorithm~\cite{Demaine2000ASI,DLMI01}, group based algorithm~\cite{Ding2011}. Among them, we implement the adaptive algorithm inside Smart SSD, for the following reasons: (1) Lucene uses the adaptive algorithm for doing list intersection. For a fair comparison, we also use it inside Smart SSD. (2) The adaptive algorithm works well in theory and practice. E.g., according to a recent study in~\cite{Ding2011}, the adaptive algorithm performs very well (than other algorithms except the group based algorithm). Although group based algorithm~\cite{Ding2011} performs better, it needs too much memory for pre-computation. However, inside Smart SSD, the memory is limited.

 \begin{algorithm}[htbp]\small
load all the $k$ lists $L_1, L_2, \cdots, L_k$ from SSD to memory\nllabel{alg1:0} (assume $L_1[0] \le L_2[0] \le ... \le L_k[0]$)\\
%result set $R$ to be empty\\
result set $R \leftarrow \varnothing$\\
set $pivot$ to the first element of $L_k$ \nllabel{alg1:2}\\

 \Repeat( access the lists in cycle:){target $=$ \emph{INVALID}}{
 let $L_i$ be the current list \nllabel{alg1:5}\\
 $\emph{successor} \leftarrow L_i.\texttt{next}(pivot)$ /*smallest element $\ge$ \emph{pivot}*/\nllabel{alg1:6}\\
    \If{$successor = pivot$}{
        increase occurrence counter and insert $pivot$ to $R$ if the count reaches $k$\nllabel{alg1:8}
    }\Else{
    $pivot \leftarrow successor$\nllabel{alg1:10}\\
    }
 }
 \textbf{return} $R$\\
 \caption{Adaptive intersection algorithm}\label{alg:AdaptiveIntersection}
 \end{algorithm}

Algorithm~\ref{alg:AdaptiveIntersection} shows the adaptive algorithm~\cite{Demaine2000ASI,DLMI01}. Every time, a \emph{pivot} value is selected (initially, it is set to the first element of $L_k$, see Line~\ref{alg1:2}). It is probed against the other lists in a round-robin fashion. Let $L_i$ be the current list where the {pivot} is probed on (line~\ref{alg1:5}). If \emph{pivot} is in $L_i$ (using binary search, line~\ref{alg1:6}), increase the counter for the {pivot} (line~\ref{alg1:8}); otherwise, update the {pivot} value to be the successor (line~\ref{alg1:10}). In either case, continue probing the next available list until the pivot is INVALID (meaning that at least one list is exhausted).



%We use an example to illustrate the Algorithm~\ref{alg:AdaptiveIntersection}. Let $k=3$, $L_1 = \{1, 10, 11, 12, 13\}$, $L_2 = \{4, 9, 10, 11, 13\}$ and $L_3 = \{8, 11, 13\}$. Initially, \emph{target} is 8, i.e., the first element in $L_3$. Then, examine $L_1$ and $L_1$.\texttt{next}(8) is 10, then set the \emph{target} to 10. Next, examine $L_2$, and $L_2$.\texttt{next}(10) is 10 (the counter for 10 is 2). Then, check $L_3$, and $L_3$.\texttt{next}(10) is 11 ($\neq 10$), but the counter is reset to 1 as it is different from the previous \emph{target} value. Then, we go to $L_1$ (circularly), and continue. For more information refer to Table~\ref{tab:runExample}.
%\begin{table}[htbp]
%\small
%\centering \caption{Running example}\label{tab:runExample}
%\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|} \hline
%
%  \textbf{step}&	\textbf{current}&	\textbf{next()}&	 \textbf{target}&	\textbf{counter}&	\textbf{R}\\\hline\hline
%  1	&-&	-&	8&	1&	\\\hline
%2&	$L_1$&	10&	10&	1&	\\\hline
%3&	$L_2$&	10&	10&	2&	\\\hline
%4&	$L_3$&	11&	11&	1&	\\\hline
%5&	$L_1$&	11&	11&	2&	\\\hline
%6&	$L_2$&	11&	13&	3&	11\\\hline
%7&	$L_3$&	13&	13&	2&	11\\\hline
%8&	$L_1$&	13&	INVALID&	3&	11,13\\\hline
%\end{tabular}
%\end{table}

\textbf{Switch between the adaptive algorithm and the sort-merge algorithm}. The performance of Algorithm~\ref{alg:AdaptiveIntersection} depends on how to find the successor of a pivot efficiently (Line~\ref{alg1:6}). We mainly use binary search in our implementation. However, when two lists are of similar sizes, linear search can even be faster than binary search~\cite{Ding2011}. Thus, in our implementation, if the size ratio of two lists is less than 4 (empirically determined), we use linear search to find the successor. In this case, the adaptive algorithm is degraded to the sort-merge algorithm.

\textbf{Remark on Lucene's implementation of list intersection and our optimizations}. We find that Lucene's implementation of list intersection is not very efficient. (1) To store several inverted lists, Lucene uses an STL-like data structure (e.g., \texttt{vector< vector<> >}), which adds a layer of indirection. Besides, every memory access involves a boundary checking which also adds some overhead. This also holds in other operations, e.g., union and difference. In our implementation, we use raw byte arrays (i.e., \texttt{char *data}) to store all the lists. All the inverted lists are stored in byte stream continuously in a big memory space, separated by document frequency in the metadata. The entry of a list can be obtained using type-conversion directly. (2) In finding the pivot, Lucene always set it to the \emph{largest} element among the current elements in all lists. Thus, Lucene applies a heap data structure to store the lists based on the current elements of the lists. Thus, every there are many heap adjustment operations (see Line 4 -- 6 in Figure~\ref{fig:doNext}). This introduces a lot of overhead. In our implementation, we set the pivot to the successor (Line~\ref{alg1:10}). In this way, we can remove the overhead of the heap adjustments. Experiments show that, our optimizations can improve the performance by 50\% (Section~\ref{sec:expResults}).

\subsection{Union}\label{sec:union}
We implement the standard sort-merge based algorithm (used in Lucene) for executing the union operation, see Algorithm~\ref{alg:mergeUnion}.

 \begin{algorithm}[htbp]\small
% \KwIn{$k$ inverted lists $L_1, L_2, \cdots, L_k$ on SSD}
% \KwOut{$L_1 \cup L_2 \cup \cdots \cup L_k$}

load all the $k$ lists $L_1, L_2, \cdots, L_k$ from SSD to memory\nllabel{alg1:0}\\
result set $R \leftarrow \varnothing$\\
let $p_i$ be a pointer for every list $L_i$ (initially $p_i \leftarrow 0$)\\
\Repeat( ){all the lists are exhausted}{
 let $minID$ be the smallest element among all $L_i[p_i]$\nllabel{alg2:min}\\
 advance $p_i$ by 1 if $L_i[p_i] = minID$\nllabel{alg2:move}\\
 insert $minID$ to $R$\nllabel{alg2:minID}
 }
 \textbf{return} $R$\\
 \caption{Merge-based union algorithm}\label{alg:mergeUnion}
 \end{algorithm}

\textbf{Remark on Algorithm~\ref{alg:mergeUnion}}. It is interesting to note that, Algorithm~\ref{alg:mergeUnion} scans all the elements in the inverted lists. And more importantly, for every qualified document ID (Line~\ref{alg2:minID}), we need $2k$ memory accesses. That is because, every time, need to compare the $L_i[p_i]$ values (for all $i$) in order to find the minimum value, i.e., Line~\ref{alg2:min}. Then, scan the $L_i[p_i]$ values \emph{again} to move $p_i$ whose $L_i[p_i]$ equals to the minimum (Line~\ref{alg2:move}). Thus, the total number of memory accesses can be estimated by: $2k\cdot |L_1 \cup L_2 \cup \cdots \cup L_k|$. E.g., let $k=2$, $L_1$ = \{10, 20, 30, 40, 50\}, $L_2$ = \{10, 21, 31, 41, 51\}. For the first result 10, we need to compare 4 times. Similarly for the rest.
Thus, the performance depends on the number of lists and result size. On average, each element in the result set is scanned $2k$ times, and in practice, $|L_1 \cup L_2 \cup \cdots \cup L_k| \approx \sum_i|L_i|$, meaning that, approximately, every list has to be accessed $2k$ times.

\textbf{Remark on Lucene's implementation of list union and our optimizations}. Lucene's implementation on union also suffers from inefficiency. (1) As pointed in Section~\ref{sec:intersection}, we removed the STL-like data structure by using raw byte array. (2) In order to find the minimum value among all $L_i[p_i]$, Lucene uses a heap-like data structure based on the current elements. This incurs a lot of overhead of heap adjustments. We removed.

\subsection{Difference}\label{sec:difference}
The difference operator is applicable for two lists, e.g., list \emph{A} and \emph{B}. Then $A-B$ finds all elements in \emph{A} but not in \emph{B}. The algorithm is trivial: for each element $e\in A$, check whether $e$ is in $B$. If yes, discard; otherwise, insert $e$ to the result set. Continue until $A$ is exhausted. This algorithm is also used in Lucene system.

In our implementation, for element checking, mainly use binary search. However, if the size ratio between two lists is less than 4, we switch to linear search (same as Line~\ref{alg1:6} in Algorithm~\ref{alg:AdaptiveIntersection}).


\textbf{Remark on Lucene's implementation of list difference and our optimizations}. Lucene's implementation of list difference is efficient enough, except the STL-like data structures, where we removed.

\subsection{Ranked Operations}\label{sec:ranked}

The above list operations (e.g., intersection) can return many results. However, end users are interested in top most relevant results, which requires two steps. (1) Similarity computation: for each qualified document $d$ in the result set, compute the similarity (or score) between $q$ and $d$, according to a ranking function; (2) Ranking: find the top-$k$ documents with the highest scores. The basic implementation will incur too much overhead. Thus, we need a careful implementation inside Smart SSD.

We review the standard Okapi BM25 model~\cite{Robertson1994,Singhal01} to determine the similarity between a query and a document.

\noindent Let,

\begin{tabular}{rl}
$qtf:$& term's frequency in query $q$ (typically 1)\\
$tf:$ & term's frequency in document $d$\\
$N:$ & total number of documents\\
$df:$ & number of documents that contain the term\\
%& (i.e., length of the term's inverted list)\\
$dl:$ & document length\\
$avdl:$ & average document length\\
\end{tabular}

\noindent Then,

\begin{displaymath}
  \begin{aligned}
    Similarity(q,d)&=\sum_{t\in q}(Similarity(t,d) \times qtf)\\
    Similarity(t,d)&=\frac{1+\ln(1+\ln(tf))}{0.8 + 0.2\frac{dl}{avdl}} \times \ln\frac{N+1}{df}\\
  \end{aligned}
\end{displaymath}




Typically, each entry in the inverted list contains document frequency (in addition to document ID and positional information).
Upon a qualified result ID is returned, the score is computed using the above equation. However, all parameters in $Similarity(t,d)$ are not query-specific, which can be \emph{pre-computed}. Thus, in our implementation, instead of storing the actual document frequency (i.e., \emph{df}), we store the score, i.e., $Similarity(t,d)$. This is important inside Smart SSD (due to the limited processor speed). Thus, it is much more efficient to obtain the similarity. That mean, the query quality will not degrade by using Smart SSD.

The remaining question is how to efficiently find the top ranked results. The standard solution is to implement a heap-like structure, and insert the results. This will incur too many memory accesses due to heap adjustment. In our implementation, we maintain the top-$k$ results at the SRAM area, then scan all the similarities to update the results in SRAM if necessary. 