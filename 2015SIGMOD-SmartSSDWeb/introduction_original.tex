

\section{Introduction}
%Para1: SSD-based search engine
Solid state drives (SSDs) have gained much momentum in the storage market because of the compelling advantages of SSDs over hard disk drives (HDDs). For example, SSDs offer one to two orders of magnitude faster random access than HDDs~\cite{Graefe2009}. Besides that, SSDs consume much less energy than HDDs~\cite{Chen2011PCM}.
As a result, large-scale search engines deploy SSDs in their storage systems to achieve short \emph{query latency} and less \emph{energy consumption}, which are two primary design objectives of search engines.
Because, (1) long query latencies can degrade user experience hence directly reduce revenues~\cite{Schurman09}, and (2) lower energy is not only environmentally friendly, but also saves monetary cost.
As an example, Baidu\footnote{\small \url{http://www.baidu.com}}, the largest search engine in China, has already adopted SSDs as their main storage to reduce query latency and energy consumption~\cite{Ruyue2010}.

Recently, a number of research studies discussed how to \emph{make full use} of SSDs in search engines, instead of just using SSDs as yet-another-faster HDDs. E.g., SSD-aware cache management~\cite{Tong2013,Wang2013ISS}, SSD-aware inverted index maintenance~\cite{Ruixuan2011}.
These pieces of research share the same goal of optimizing search engine systems, while treating SSDs as storage-\emph{only} devices. In this way, data storage and computing is rigorously \emph{separated}: data is stored on SSDs, while computing is executed at host machines. Upon computation, data is transferred from SSDs to the host machines through host I/O interfaces (typically SAS/SATA or PCIe).

However, recent studies indicate that this computing paradigm of ``\emph{move data closer to code}'' cannot make full use of SSDs. E.g., Baidu's SSD storage system can only use 40\% of raw flash's bandwidth~\cite{Ouyang2014SSF}. That is because, (1) SSDs generally provide higher internal bandwidth than the host I/O interface bandwidth. In our SSD, the internal bandwidth is around 3.2 GB/s, while the host I/O bandwidth is around 750 MB/s. However, data must be transferred to the host via the host I/O interface, %whose bandwidth can be easily saturated, 
resulting in a waste of SSDs' high internal bandwidth; (2) SSDs, in general, provide high computing capabilities, (for executing complex FTL firmware code~\cite{Chung2009SFT}), which are ignored by search engines where SSDs are treated as storage-\emph{only} devices. In short, a modern SSD is not only a storage unit, but also a compute unit.

This work proposes to apply \emph{SSD In-Storage Computing (ISC)} to reduce query latency and energy consumption in search engines. The main idea is to offload some operations (e.g., inverted list intersection) from host search engines to SSD devices, to take advantage of the high internal bandwidth, low I/O latency, and computing power. Such SSD devices are also called \emph{Smart SSDs}~\cite{Do2013QPS} because they can execute application specific programs. In this work, we use ``Smart SSD'' and ``SSD In-Storage Computing'' interchangeably.
Smart SSDs changed the traditional computing paradigm to ``\emph{move code closer to data}'' (or generally near-data processing~\cite{Balasubramonian14}). In addition to the performance improvement, Smart SSDs significantly reduce energy consumption due to less data movement and power-efficient processors. As a result, SSD ISC is a very promising solution to \emph{make full use} of modern SSDs in search engines. SSD ISC is also attractive to industry. E.g., IBM applies Smart SSDs in their Blue Gene storage systems~\cite{Julich13}.

The challenge is to determine what query processing operations in search engines can be cost-effectively offloaded to Smart SSDs. 
A basic principle is that the output size of the operation should be smaller than the input size, otherwise, Smart SSDs cannot save any data movement. Thus, we identified five commonly used operations in search engines that could potentially benefit from Smart SSDs. They are, \textsf{intersection}, \textsf{ranked intersection}, \textsf{ranked union}, \textsf{difference}, and \textsf{ranked difference}. We conducted extensive experiments to study the tradeoffs of the query offloading.

This work focuses on Lucene, instead of general search engines for two reasons: (1) Lucene is open-source, and it is widely adopted in industry, e.g., LinkedIn~\cite{Hien2013} and and Twitter~\cite{Busch2012ERS}. (2) It is interesting to know how Smart SSDs benefit the entire search engine system (not only algorithms).
Even though, many techniques can be applicable to other search engines.

This work made the following contributions:
\begin{itemize}
  \item To the best of our knowledge, this is the first work which explores SSD in-storage computing in search engine area. We identified several key operations of Lucene (and any search engine) and offloaded them to Smart SSDs.
  \item With the collaboration of an SSD vendor, we implemented all the operations efficiently inside real SSDs.
  \item We integrated Smart SSDs with a popular open-source search engine -- Apache Lucene.
  \item We conducted extensive experiments on both real dataset and synthetic dataset to evaluate the system performance.
  The results are very promising: Smart SSDs can reduce the query latency by a factor of 2-3$\times$ and energy consumption by 6-10$\times$ for most operations.
\end{itemize}

%This work explores how to apply Smart SSDs to Apache Lucene\footnote{\small\url{http://lucene.apache.org}} (a popular open-source search engine).
%The major research issue is to determine what query processing steps of Lucene can be cost-effectively offloaded to Smart SSDs.
%To answer this question, we identified five commonly used operations in Lucene (and any search engine) that could potentially benefit from Smart SSDs.
%They are \textsf{intersection}, \textsf{ranked intersection}, \textsf{ranked union}, \textsf{difference}, and \textsf{ranked difference}.
%With the collaboration of an SSD vendor, we codesigned the host Lucene system and the Smart SSD, by offloading the operations to the Smart SSD.
%Finally, we conducted extensive experiments to evaluate the performance and tradeoffs by using both synthetic datasets and real datasets (provided by a commercial large-scale search engine company).
%The experimental results show that Smart SSDs reduce the query latency by a factor of 2-3$\times$ and energy consumption by 6-10$\times$ for most of the aforementioned operations.
%

%The rest of this paper is organized as follows.
%Section~\ref{sec:background} provides an overview of Smart SSDs, and Lucene's search architecture.
%Section~\ref{sec:design} presents the integrated system design space and architecture of the Smart SSD and Lucene.
%Section~\ref{sec:implementation} details the implementation of query offloading.
%Section~\ref{sec:expSetup} and Section~\ref{sec:expResults} show the evaluation, and discuss the tradeoffs of the query offloading.
%Section~\ref{sec:relatedWork} discusses some related studies of this work.
%Section~\ref{sec:conclusion} concludes our work.



%JG: do not motivate like this, the paper will be rejected by an energy people
%%Para1: describe briefly what's a search engine, what're the problems
%Large-scale search engines process billions of queries over billions of documents per day\footnote{\small \url{http://www.internetlivestats.com/google-search-statistics}}. There are two primary design objectives of search engines:
%(1) Short query latency. Search engines usually follow a service level agreement (SLA) to return results in a certain amount of time, e.g., 500 ms. Long query latencies can degrade user experience and directly reduce revenues~\cite{Schurman09}.
%(2) Low energy. Search engines consume tremendous amount of energy, e.g., Google consumes around 12.5 million watts per day for the web search service~\cite{GoogleEnergy11}. Lower energy is not only environmentally friendly, but also saves monetary cost.
%
%%Thus, Google (and any search engines) designed energy efficient data centers to minimize the energy consumption per query\footnote{\small\url{http://googleblog.blogspot.com/2009/01/powering-google-search.html}}.
%
%In search engine community, many efficient query processing techniques were proposed to reduce query latency, which can also implicitly reduce energy consumption\footnote{\small Energy is a product of power (in watts) and latency.}. For instance, efficient list intersection algorithm~\cite{vldb2011}.
%
%
%Large search engines now have to process thousands of queries per second over tens of billions of documents, how important it is to our daily life
%There are two primary design objectives for search engines:
%(1) Achieving consistently low response times. Because search engine operates under a service level agreement (SLA), e.g., return return less than 0.2s
%(2) Answering the query with minimum energy. Google also cares more about energy, e.g., (http://googleblog.blogspot.com/2009/01/powering-google-search.html) "...information technology consumes an increasing amount of energy, and Google takes this impact seriously. That's why we have designed and built the most energy efficient data centers in the world, which means the energy used per Google search is minimal"
%
%Para2: describe the existing work. And their drawbacks.
%Many studies were devoted to solve the problem. E.g., in query efficiency side: design efficient algorithms, e.g., list intersection algorithms, top-k ranking algorithms, caching, parallel; in energy efficiency side: design energy efficient data centers, e.g., better cooling techniques.
%Drawbacks: existing solutions usually seperate query efficiency and energy efficiency. That is, focus on one side.
%
%combine both words, energy efficient side do not solve ...
%记住, energy = latency * power，所以，如果latency降低的话，energy也会降低；我们argue说，如果explicitly 考虑的话，会更好
%
%Para3: our approach.
%This papaer proposes a novel approach (SSD ISC) to improve both sides: improve query efficiency, but also reduce energy consumption.
%Explain SSD ISC more and more, and how can we achieve the goals.
%
%Para4: challenges
%- Determine what query processing logic should be offloaded
%- how we decide...mention our criteria
%
%Para5: we focus on Lucene, instead of general...
%
%Para5: Our contributions.
%- The first work to solve the search engine problem using SSD ISC, identify several key operations to offload to SSDs
%- More efficient implementations on real SSDs
%- Integration with Lucene
%- Extensive experiments, show that, reduce latency by 2-3X, energy by 6-10X.
%
%Para6: Paper organization.


%Solid state drives (SSDs) have gained much momentum in the storage market because of the compelling advantages of SSDs over hard disk drives (HDDs). E.g., SSDs are one to two orders of magnitude faster than HDDs in random reads~\cite{AthanassoulisACGS10}. In past years, many research studies discussed how to \emph{make full use} of SSDs in high level software systems (e.g., database systems) instead of just using SSDs as yet-another-faster HDDs.
%E.g., SSD-aware Btrees~\cite{Li2010TIS}, SSD-aware buffer management~\cite{Do2011TDB}. These pieces of research share the same goal of optimizing software systems, while treating SSDs as storage-\emph{only} devices. In this way, data storage and computing is rigorously \emph{separated}: data is stored on SSDs, while computing is executed at host machines. Upon computation, data is transferred from SSDs to the host machines through host I/O interfaces (typically SAS/SATA or PCIe).
%
%However, recent studies indicate that this computing paradigm of ``\emph{move data closer to code}'' cannot make full use of SSDs for two main reasons~\cite{Do2013QPS,WoodsIA14}. (1) SSDs generally provide higher internal bandwidth than the host I/O interface bandwidth. In our SSD, the internal bandwidth is around 3.2 GB/s, while the host I/O bandwidth is around 750 MB/s. However, since data must be transferred to the host via the host I/O interface, whose bandwidth can be easily saturated by data-intensive applications, resulting in a waste of SSDs' high internal bandwidth; (2) SSDs, in general, provide high computing capabilities (for executing complex FTL firmware code~\cite{Chung2009SFT}), which are ignored by high level systems where SSDs are treated as storage-\emph{only} devices.
%
%
%Thus, Do et. al proposed an approach of integrating storage and computing inside SSDs, called \emph{Smart SSDs}~\cite{Do2013QPS} or \emph{SSD In-Storage Computing (ISC)}\footnote{\small In this work, we use the term ``Smart SSD'' and ``SSD In-Storage Computing'' interchangeably.}. Smart SSDs allow the execution of application specific code (e.g., database scan and aggregation) inside SSDs, to take advantage of the high internal bandwidth, low I/O latency, and computing power. This ISC approach changed the traditional computing paradigm to ``\emph{move code closer to data}'' (or generally near-data processing~\cite{Balasubramonian14}). In addition to the performance improvement, Smart SSDs significantly reduce energy consumption due to less data movement and its power-efficient processors. As a result, executing application logic inside Smart SSDs is a very promising solution to \emph{make full use} of modern SSDs. It also attractive to industry. E.g., IBM applies Smart SSDs in their Blue Gene storage systems~\cite{Julich13}.
%
%
%This work explores how to apply Smart SSDs to Apache Lucene\footnote{\small\url{http://lucene.apache.org}} (a popular open-source search engine).
%The major research issue is to determine what query processing steps of Lucene can be cost-effectively offloaded to Smart SSDs.
%To answer this question, we identified five commonly used operations in Lucene (and any search engine) that could potentially benefit from Smart SSDs.
%They are \textsf{intersection}, \textsf{ranked intersection}, \textsf{ranked union}, \textsf{difference}, and \textsf{ranked difference}.
%With the collaboration of an SSD vendor, we codesigned the host Lucene system and the Smart SSD, by offloading the operations to the Smart SSD.
%Finally, we conducted extensive experiments to evaluate the performance and tradeoffs by using both synthetic datasets and real datasets (provided by a commercial large-scale search engine company).
%The experimental results show that Smart SSDs reduce the query latency by a factor of 2-3$\times$ and energy consumption by 6-10$\times$ for most of the aforementioned operations.
%
%This work focuses on Lucene, instead of general search engines for two reasons: (1) Lucene is open-source, and it is widely adopted in industry, e.g., LinkedIn~\cite{Hien2013} and and Twitter~\cite{Busch2012ERS}. (2) It is interesting to know how Smart SSDs benefit the entire search engine system (not only algorithms).
%Even though, many techniques can be applicable to other search engines.
%
%To the best of our knowledge, this is the first work which explores SSD in-storage computing in the search engine area. We demonstrate that, SSD ISC is promising to reduce query latency as well as energy consumption, which is very crucial to Lucene and any search engines. 


%To the best of our knowledge, this is the first work to explore SSD in-storage computing to search engine area. With the advent of SSDs, the boundary between software systems (i.e., Lucene in our case) and SSD storage devices is getting blurrier ~\cite{Steve12}. Thus, to fully utilize the advantages of SSD to search engine, this work shows that it is beneficial to consider a system co-design approach between search engine and SSD. In this way, computing power and storage are tightly integrated to improve the overall performance.

%The rest of this paper is organized as follows.
%Section~\ref{sec:background} provides an overview of Smart SSDs, and Lucene's search architecture.
%Section~\ref{sec:design} presents the integrated system design space and architecture of the Smart SSD and Lucene.
%Section~\ref{sec:implementation} details the implementation of query offloading.
%Section~\ref{sec:expSetup} and Section~\ref{sec:expResults} show the evaluation, and discuss the tradeoffs of the query offloading.
%Section~\ref{sec:relatedWork} discusses some related studies of this work.
%Section~\ref{sec:conclusion} concludes our work.

